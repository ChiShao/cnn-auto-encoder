{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, LeakyReLU\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "from matplotlib.colors import hsv_to_rgb, rgb_to_hsv\n",
    "from PIL import Image, ImageEnhance\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "from sklearn import svm\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "import trainer.parse_file_list\n",
    "from trainer.gpu_utils import ModelCkptMultiGPU, get_available_gpus\n",
    "from trainer.model_ckpt_gc import ModelCheckpointGC\n",
    "from trainer.plot_utils import plot_hist, plot_mvtec, plot_samples, savefig\n",
    "\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "# from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input\n",
    "# from tensorflow.keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for redproducible results\n",
    "RAND = 42\n",
    "np.random.seed(RAND)\n",
    "tf.set_random_seed(RAND)\n",
    "random.seed(RAND)\n",
    "\n",
    "# set CONSTANTS\n",
    "USE_CASE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "args  = SimpleNamespace(use_case=\"spm\",\n",
    "                        datadir=\"gs://anomaly-detection-playground-data/\",\n",
    "                       logdir=\"gs://anomaly-detection-playground/logs/DEBUG_AD\",\n",
    "                       ckptdir=\"gs://anomaly-detection-playground/ckpts/DEBUG_AD\",\n",
    "                       imgdir=\"gs://anomaly-detection-playground/imgs/DEBUG_AD\",\n",
    "                       evaldir=\"gs://anomaly-detection-playground/eval/DEBUG_AD\",\n",
    "                       epochs=256,\n",
    "                       batch_size=128,\n",
    "                       filters=[16,16,32,32,64,64,128,128,256],\n",
    "                       ldim=512)\n",
    "\n",
    "global USE_CASE\n",
    "USE_CASE = args.use_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_file_names(dir_path):\n",
    "    \"\"\"Returns a list of absolute file paths for relative dir input with all relevant file names.\"\"\"\n",
    "    return [os.path.join(dir_path, p) for p in file_io.list_directory(dir_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_to_data(data_path, mode):\n",
    "    return os.path.join(data_path, USE_CASE, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    normal_path = os.path.join(data_path, USE_CASE, \"train\", \"good\")\n",
    "    normal_file_names = read_image_file_names(normal_path)\n",
    "    random.shuffle(normal_file_names)\n",
    "\n",
    "    train_validation_split = (len(normal_file_names) // 10) * 8  # equals .85\n",
    "    train_file_names, validation_file_names = normal_file_names[\n",
    "        :train_validation_split], normal_file_names[train_validation_split:]\n",
    "\n",
    "    test_path = os.path.join(data_path, USE_CASE, \"test\")\n",
    "\n",
    "    normal_test_file_names = read_image_file_names(\n",
    "        os.path.join(test_path, \"good\"))\n",
    "\n",
    "    anomaly_test_file_names = anomaly_train_file_names = []\n",
    "    # iterate of test list dir because anomalies are named dynamically\n",
    "    for i, p in enumerate(filter(lambda x: x != \"good\", file_io.list_directory(test_path))):\n",
    "        if i == 0:\n",
    "            anomaly_test_file_names += read_image_file_names(\n",
    "                os.path.join(test_path, p))\n",
    "        else:\n",
    "            anomaly_train_file_names += read_image_file_names(\n",
    "                os.path.join(test_path, p))\n",
    "\n",
    "    print(\"Splitting data:\\n%4d Normal Train Samples\\n%4d Normal Validation Samples\\n%4d Normal Test Samples\\n%4d Anomaly Train Samples\\n%4d Anomaly Test Samples\" % (\n",
    "        len(train_file_names), len(validation_file_names), len(normal_test_file_names), len(anomaly_train_file_names), len(anomaly_test_file_names)))\n",
    "\n",
    "    return train_file_names, validation_file_names, normal_test_file_names, anomaly_train_file_names, anomaly_test_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://anomaly-detection-playground-data/no_damage.txt\n",
      "Splitting data:\n",
      "261792 Normal Train Samples\n",
      "65449 Normal Validation Samples\n",
      "109082 Normal Test Samples\n",
      "2063 Anomaly Train Samples\n",
      " 689 Anomaly Test Samples\n"
     ]
    }
   ],
   "source": [
    "# define train params\n",
    "target_size = (256, 256, 3)\n",
    "print(os.path.join(args.datadir, \"no_damage.txt\"))\n",
    "\n",
    "train_file_names, validation_file_names, normal_test_file_names, anomaly_train_file_names, anomaly_test_file_names = trainer.parse_file_list.load_data(os.path.join(args.datadir, \"no_damage.txt\"),\n",
    "                                                                                                                                                       os.path.join(args.datadir, \"damage_encoded.txt\"))\n",
    "# train_generator, validation_generator, test_generator = preproc_data(\n",
    "#     input_shape, batch_size, args.datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_img_generator(file_paths, batch_size, target_size, preproc=True, input_only=False):\n",
    "    # yields batch_size-d arrays of images indefinetly\n",
    "    # order of images is random.\n",
    "    # ONLY FOR TRAIN AND EVALUATE PURPOSES SUITABLE\n",
    "    # \"\"\"DEPRECATED: keras img preproc is used for train purposes.\n",
    "    # Still used for evaluation purposes.\"\"\"\n",
    "\n",
    "    while True:\n",
    "        inds = (np.random.randint(0, len(file_paths), batch_size))\n",
    "        imgs = np.array([])\n",
    "        for i in inds:\n",
    "            fp = file_paths[i]\n",
    "            # print(fp)\n",
    "            if file_io.file_exists(fp):\n",
    "                img = filepath_to_image(fp)\n",
    "                img = img.resize(target_size[:-1], Image.BICUBIC)  # BGR -> RGB\n",
    "                if preproc:\n",
    "                    np_img = preproc_img(img, flip_top_bottom=False, hsv=None)\n",
    "                else:\n",
    "                    np_img = np.array(img) / 255\n",
    "\n",
    "                imgs = np.concatenate(\n",
    "                    (imgs, np.array([np_img]))) if imgs.size > 0 else np.array([np_img])\n",
    "        if not input_only:\n",
    "            yield imgs, imgs\n",
    "        else:\n",
    "            yield imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filepath_to_image(fp):\n",
    "    with file_io.FileIO(fp, mode=\"rb\") as f:\n",
    "        img = Image.open(f)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand(a=0, b=1):\n",
    "    \"\"\":returns random value between a and b\"\"\"\n",
    "    return np.random.rand()*(b-a) + a\n",
    "\n",
    "\n",
    "def preproc_img(image, flip_top_bottom=True, flip_left_right=True, brightness=True, contrast=True, hsv=(.1, 1.5, 1.5)):\n",
    "    \"\"\"https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/utils.py\"\"\"\n",
    "    # flip image or not\n",
    "    if flip_left_right and rand() < .5:\n",
    "        image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    if flip_top_bottom and rand() < .5:\n",
    "        image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "    # image brighDtness enhancer\n",
    "    if brightness:\n",
    "        brightness = ImageEnhance.Brightness(image)\n",
    "        image = brightness.enhance(rand(.5, 1.5))\n",
    "\n",
    "    # increase or decrease contrast\n",
    "    if contrast:\n",
    "        contrast = ImageEnhance.Contrast(image)\n",
    "        image = contrast.enhance(rand(.5, 1.5))\n",
    "\n",
    "    image = np.array(image) / 255.\n",
    "\n",
    "    if hsv != None:\n",
    "        hue, sat, val = hsv\n",
    "        # distort image\n",
    "        hue = rand(-hue, hue)\n",
    "        sat = rand(1, sat) if rand() < .5 else 1/rand(1, sat)\n",
    "        val = rand(1, val) if rand() < .5 else 1 / rand(1, val)\n",
    "\n",
    "        x = rgb_to_hsv(image)\n",
    "        x[..., 0] += hue\n",
    "        x[..., 0][x[..., 0] > 1] -= 1\n",
    "        x[..., 0][x[..., 0] < 0] += 1\n",
    "        x[..., 1] *= sat\n",
    "        x[..., 2] *= val\n",
    "        x[x > 1] = 1\n",
    "        x[x < 0] = 0\n",
    "        image = hsv_to_rgb(x)\n",
    "    return image  # numpy array, 0 to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_generator(file_paths, n, target_size):\n",
    "    # returns generator of n images\n",
    "    if len(file_paths) < n:\n",
    "        # prevent an IndexError\n",
    "        n = len(file_paths)\n",
    "\n",
    "    for i in range(n):\n",
    "        fp = file_paths[i]\n",
    "        if file_io.file_exists(fp):\n",
    "            img = filepath_to_image(fp)\n",
    "            img = img.resize(target_size[:-1], Image.BICUBIC)\n",
    "            yield np.array(img) / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ae(autoencoder):\n",
    "    encoder_layer = autoencoder.get_layer(\"encoder\")\n",
    "    # this model maps an input to its encoded representation; Big image to small rep\n",
    "    encoder = Model(\n",
    "        inputs=autoencoder.get_input_at(0), outputs=encoder_layer.output)\n",
    "\n",
    "    # create a placeholder for an encoded (ENCODING_DIM-dimensional) input\n",
    "    encoded_input = Input(shape=encoder_layer.output_shape[1:])\n",
    "\n",
    "    # getting the middle of the autoencoder\n",
    "    start = (len(autoencoder.layers))//2\n",
    "    decoder = autoencoder.layers[-start](encoded_input)\n",
    "    # stacking the decoder layers\n",
    "    for i in range(start-1, 0, -1):\n",
    "        decoder = autoencoder.layers[-i](decoder)\n",
    "\n",
    "    # create the decoder model; \"<\": encoded(small) representation to big image\n",
    "    decoder = Model(encoded_input, decoder)\n",
    "    return encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_conv_ae(filters, input_shape=(256, 256, 3)):\n",
    "    n = 10\n",
    "    print(filters)\n",
    "    if len(filters) != n:\n",
    "        raise ValueError(\"%d Filters must be given. Sorry.\" % n)\n",
    "    # this is our input placeholder\n",
    "    input_img = Input(shape=input_shape)\n",
    "    # layer between input and middle layer\n",
    "    i = 0\n",
    "    encode = Conv2D(\n",
    "        filters[i], (4, 4), strides=(2, 2), padding=\"same\"\n",
    "    )(input_img)\n",
    "    encode = LeakyReLU(0.2)(encode)\n",
    "    i += 1\n",
    "    encode = Conv2D(\n",
    "        filters[i], (4, 4), strides=(2, 2),  padding=\"same\"\n",
    "    )(encode)\n",
    "    encode = LeakyReLU(0.2)(encode)\n",
    "\n",
    "    i += 1\n",
    "    encode = Conv2D(\n",
    "        filters[i], (3, 3), strides=(1, 1),  padding=\"same\"\n",
    "    )(encode)\n",
    "    encode = LeakyReLU(0.2)(encode)\n",
    "    i += 1\n",
    "    encode = Conv2D(\n",
    "        filters[i], (4, 4), strides=(2, 2),  padding=\"same\"\n",
    "    )(encode)\n",
    "    encode = LeakyReLU(0.2)(encode)\n",
    "    i += 1\n",
    "    encode = Conv2D(\n",
    "        filters[i], (3, 3), strides=(1, 1),  padding=\"same\"\n",
    "    )(encode)\n",
    "    encode = LeakyReLU(0.2)(encode)\n",
    "    i += 1\n",
    "    encode = Conv2D(\n",
    "        filters[i], (4, 4), strides=(2, 2),  padding=\"same\"\n",
    "    )(encode)\n",
    "    encode = LeakyReLU(0.2)(encode)\n",
    "    i += 1\n",
    "    encode = Conv2D(\n",
    "        filters[i], (4, 4), strides=(2, 2),  padding=\"same\"\n",
    "    )(encode)\n",
    "    encode = LeakyReLU(0.2)(encode)\n",
    "    i += 1\n",
    "    encode = Conv2D(\n",
    "        filters[i], (3, 3), strides=(1, 1),  padding=\"same\"\n",
    "    )(encode)\n",
    "    encode = LeakyReLU(0.2)(encode)\n",
    "    i += 1\n",
    "    encode = Conv2D(\n",
    "        filters[i], (4, 4), strides=(2, 2),  padding=\"same\"\n",
    "    )(encode)\n",
    "    encode = LeakyReLU(0.2)(encode)\n",
    "    i += 1\n",
    "    # \"encoded\" is the encoded representation of the input, middle layer of the aue\n",
    "    encoded = Conv2D(\n",
    "        filters[i], (4, 4), strides=(1, 1),  name=\"encoder\"\n",
    "    )(encode)\n",
    "\n",
    "    i -= 1\n",
    "    # layer between middle and output layer\n",
    "    decode = Conv2DTranspose(filters[i], (4, 4), strides=(1, 1), activation=\"relu\")(\n",
    "        encoded\n",
    "    )\n",
    "    decode = LeakyReLU(0.2)(decode)\n",
    "    i -= 1\n",
    "    decode = Conv2DTranspose(\n",
    "        filters[i], (4, 4), strides=(2, 2),  padding=\"same\"\n",
    "    )(decode)\n",
    "    decode = LeakyReLU(0.2)(decode)\n",
    "    i -= 1\n",
    "    decode = Conv2DTranspose(\n",
    "        filters[i], (3, 3), strides=(1, 1),  padding=\"same\"\n",
    "    )(decode)\n",
    "    decode = LeakyReLU(0.2)(decode)\n",
    "    i -= 1\n",
    "    decode = Conv2DTranspose(\n",
    "        filters[i], (4, 4), strides=(2, 2),  padding=\"same\"\n",
    "    )(decode)\n",
    "    decode = LeakyReLU(0.2)(decode)\n",
    "    i -= 1\n",
    "    decode = Conv2DTranspose(\n",
    "        filters[i], (4, 4), strides=(2, 2),  padding=\"same\"\n",
    "    )(decode)\n",
    "    decode = LeakyReLU(0.2)(decode)\n",
    "    i -= 1\n",
    "    decode = Conv2DTranspose(\n",
    "        filters[i], (4, 4), strides=(2, 2),  padding=\"same\"\n",
    "    )(decode)\n",
    "    decode = LeakyReLU(0.2)(decode)\n",
    "    i -= 1\n",
    "    decode = Conv2DTranspose(\n",
    "        filters[i], (3, 3), strides=(1, 1),  padding=\"same\"\n",
    "    )(decode)\n",
    "    i -= 1\n",
    "    decode = LeakyReLU(0.2)(decode)\n",
    "    decode = Conv2DTranspose(\n",
    "        filters[i], (4, 4), strides=(2, 2),  padding=\"same\"\n",
    "    )(decode)\n",
    "    decode = LeakyReLU(0.2)(decode)\n",
    "    i -= 1\n",
    "    decode = Conv2DTranspose(\n",
    "        filters[i], (4, 4), strides=(2, 2),  padding=\"same\"\n",
    "    )(decode)\n",
    "    decode = LeakyReLU(0.2)(decode)\n",
    "    \n",
    "    decoded = Conv2D(\n",
    "        input_shape[-1], (3, 3), activation=\"sigmoid\", padding=\"same\"\n",
    "    )(decode)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder_single = Model(inputs=input_img, outputs=decoded)\n",
    "    n_gpus = len(get_available_gpus())\n",
    "    if n_gpus > 1:\n",
    "        autoencoder = multi_gpu_model(autoencoder_single, n_gpus)\n",
    "        print(\"Autoencoder is trained on %d GPUs\" % n_gpus)\n",
    "    else:\n",
    "        autoencoder = autoencoder_single\n",
    "        print(\"Autoencoder is trained on a single GPU\")\n",
    "    \n",
    "    encoder, decoder = split_ae(autoencoder_single)\n",
    "\n",
    "    # build (aka \"compile\") the model\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    autoencoder_single.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return autoencoder, autoencoder_single, encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(train_file_names, validation_file_names, anomaly_train_file_names, args,  target_size=(256, 256, 3)):\n",
    "    print(\"Writing logs to %s\" % args.logdir)\n",
    "    train_generator = train_img_generator(\n",
    "        train_file_names, args.batch_size, target_size)\n",
    "    validation_generator = train_img_generator(\n",
    "        validation_file_names, args.batch_size, target_size, preproc=False)\n",
    "\n",
    "    print(\"Training Autoencoder for %s feature extraction...\" % USE_CASE)\n",
    "    d = args.ldim\n",
    "    batch_size = args.batch_size\n",
    "    filters = args.filters + [d]  # [16, 16, 16, 32, 64, 64, 32, 32, d]\n",
    "\n",
    "    ae, ae_single, encoder, decoder = build_conv_ae(\n",
    "        input_shape=target_size, filters=filters)\n",
    "    # define callbacks for logging and optimized training and ckpt saving\n",
    "\n",
    "    earlyStopping = EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=20, verbose=1, mode=\"min\", min_delta=(1/10**5)\n",
    "    )\n",
    "    # saves model which was trained on a single cpu since saving the other model threw some kind of error\n",
    "    checkpoint_format = os.path.join(\n",
    "        args.ckptdir, 'ep{epoch:04d}-loss{loss:.6f}-val_loss{val_loss:.6f}.h5')\n",
    "\n",
    "    mcp_save = ModelCheckpointGC(\n",
    "        checkpoint_format, ae_single, save_best_only=True, verbose=1, monitor=\"val_loss\", mode=\"min\"\n",
    "    )\n",
    "    reduce_lr_loss = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.2, patience=7, verbose=1, mode=\"min\"\n",
    "    )\n",
    "\n",
    "    tb = TensorBoard(\n",
    "        log_dir=args.logdir, histogram_freq=0, write_graph=True, write_images=True\n",
    "    )\n",
    "\n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        ae_single.summary()\n",
    "    summary = f.getvalue()\n",
    "    print(summary)\n",
    "\n",
    "    with file_io.FileIO(os.path.join(args.logdir, \"train-specs.txt\"), mode=\"w\") as f:\n",
    "        f.write(\"\\n\".join((\"Filters: {}\".format(args.filters),\n",
    "                           \"Batch Size: %d\" % batch_size,\n",
    "                           \"Latent Space Dim: %d\" % d,\n",
    "                           \"Auto Encoder Network %s\" % summary)))\n",
    "\n",
    "    train_length = len(train_file_names)\n",
    "    val_length = len(validation_file_names)\n",
    "\n",
    "    ae.fit_generator(\n",
    "        generator=train_generator,  # needs to produce data infinitely\n",
    "        epochs=args.epochs,\n",
    "        # every element once on average\n",
    "        steps_per_epoch=train_length//batch_size,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=val_length//batch_size,\n",
    "        callbacks=[mcp_save, earlyStopping, reduce_lr_loss, tb]\n",
    "    )\n",
    "\n",
    "    return ae, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to gs://anomaly-detection-playground/logs/DEBUG_AD\n",
      "Training Autoencoder for spm feature extraction...\n",
      "[16, 16, 32, 32, 64, 64, 128, 128, 256, 512]\n",
      "Autoencoder is trained on 2 GPUs\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_101 (Conv2D)          (None, 128, 128, 16)      784       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_181 (LeakyReLU)  (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_102 (Conv2D)          (None, 64, 64, 16)        4112      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_182 (LeakyReLU)  (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_103 (Conv2D)          (None, 64, 64, 32)        4640      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_183 (LeakyReLU)  (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_104 (Conv2D)          (None, 32, 32, 32)        16416     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_184 (LeakyReLU)  (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_105 (Conv2D)          (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_185 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_106 (Conv2D)          (None, 16, 16, 64)        65600     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_186 (LeakyReLU)  (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_107 (Conv2D)          (None, 8, 8, 128)         131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_187 (LeakyReLU)  (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_108 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_188 (LeakyReLU)  (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_109 (Conv2D)          (None, 4, 4, 256)         524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_189 (LeakyReLU)  (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Conv2D)             (None, 1, 1, 512)         2097664   \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_91 (Conv2DT (None, 4, 4, 256)         2097408   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_190 (LeakyReLU)  (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_92 (Conv2DT (None, 8, 8, 128)         524416    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_191 (LeakyReLU)  (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_93 (Conv2DT (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_192 (LeakyReLU)  (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_94 (Conv2DT (None, 16, 16, 64)        131136    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_193 (LeakyReLU)  (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_95 (Conv2DT (None, 32, 32, 64)        65600     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_194 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_96 (Conv2DT (None, 64, 64, 32)        32800     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_195 (LeakyReLU)  (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_97 (Conv2DT (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_196 (LeakyReLU)  (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_98 (Conv2DT (None, 128, 128, 16)      8208      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_197 (LeakyReLU)  (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_99 (Conv2DT (None, 256, 256, 16)      4112      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_198 (LeakyReLU)  (None, 256, 256, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_110 (Conv2D)          (None, 256, 256, 3)       435       \n",
      "=================================================================\n",
      "Total params: 6,031,987\n",
      "Trainable params: 6,031,987\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 256, 256, 3)  0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 256, 256, 3)  0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_23 (Model)                (None, 256, 256, 3)  6031987     lambda_21[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Concatenate)        (None, 256, 256, 3)  0           model_23[1][0]                   \n",
      "                                                                 model_23[2][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,031,987\n",
      "Trainable params: 6,031,987\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/256\n",
      "   1/2045 [..............................] - ETA: 29:16:08 - loss: 0.1239"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-69-e461bd5fc913>\", line 2, in <module>\n",
      "    train_file_names, validation_file_names, anomaly_train_file_names,  args,  target_size=target_size)\n",
      "  File \"<ipython-input-68-ab7ba0aae97d>\", line 57, in train_ae\n",
      "    callbacks=[mcp_save, earlyStopping, reduce_lr_loss, tb]\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\", line 1418, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\", line 181, in fit_generator\n",
      "    generator_output = next(output_generator)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\", line 685, in get\n",
      "    inputs = self.queue.get(block=True).get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 602, in get\n",
      "    self.wait(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 599, in wait\n",
      "    self._event.wait(timeout)\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 549, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 293, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1454, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1415, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 167, in findsource\n",
      "    file = getsourcefile(object) or getfile(object)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 671, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 708, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "ae, _, _ = train_ae(\n",
    "    train_file_names, validation_file_names, anomaly_train_file_names,  args,  target_size=target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_generator(model, gen):\n",
    "    predictions = []\n",
    "    for g in gen:\n",
    "        pred = model.predict(np.array([g]), batch_size=1)\n",
    "        # pred[0], because prediction happens on an array, hence result has also length one\n",
    "        predictions.append(pred[0])\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ae, loss_boundary, oc_svm, score_boundary, X_normal_test, X_anomaly_test, img_dir, target_size):\n",
    "    # load the data for evaluation purposes\n",
    "    print(\"Evaluating feature extractor...\")\n",
    "    # eval_train = model.evaluate_generator(\n",
    "    #     train_img_generator(X_normal_train, 8), steps=len(X_normal_train))\n",
    "    batch_size = 8\n",
    "\n",
    "    eval_test = ae.evaluate_generator(\n",
    "        train_img_generator(X_normal_test, batch_size, target_size, preproc=False), steps=len(X_normal_test)//batch_size)\n",
    "    # print(\"Feature extractor train loss: %f\" % eval_train)\n",
    "\n",
    "    print(\"Feature extractor  test loss: %f\" % eval_test)\n",
    "\n",
    "    decoded_samples_normal = predict_from_generator(\n",
    "        ae, img_generator(X_normal_test, 8, target_size))\n",
    "\n",
    "    plot_samples(\n",
    "        img_generator(X_normal_test, 8, target_size),\n",
    "        decoded_samples_normal,\n",
    "        plot_mvtec,\n",
    "        os.path.join(img_dir, \"rec-normals.png\"),\n",
    "    )\n",
    "\n",
    "    decoded_samples_anomaly = predict_from_generator(\n",
    "        ae, img_generator(X_anomaly_test, batch_size, target_size))\n",
    "\n",
    "    plot_samples(\n",
    "        img_generator(X_anomaly_test, batch_size, target_size),\n",
    "        decoded_samples_anomaly,\n",
    "        plot_mvtec,\n",
    "        os.path.join(img_dir, \"rec-anomalies.png\"),\n",
    "    )\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    print(\"Evaluating the loss based approach\")\n",
    "    metrics[\"loss\"] = eval_loss(model=ae, X_normal=X_normal_test,\n",
    "                                X_anomaly=X_anomaly_test, loss_boundary=loss_boundary, img_dir=img_dir, target_size=target_size)\n",
    "    print(\"Evaluating the OC SVM\")\n",
    "\n",
    "    encoder, _ = split_ae(ae)\n",
    "    metrics[\"svm\"] = eval_svm(encoder, oc_svm, score_boundary,\n",
    "                              X_normal_test, X_anomaly_test, img_dir, target_size, batch_size)\n",
    "    print(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(TP, TN, FP, FN):\n",
    "    try:\n",
    "        precision = TP / (TP + FP)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0\n",
    "    try:\n",
    "        recall = TP / (TP + FN)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0\n",
    "\n",
    "    accuracy = (TN + TP) / (TP + TN + FN + FP)\n",
    "    TPR = recall\n",
    "    TNR = TN / (TN + FP)\n",
    "    FNR = 1 - TPR\n",
    "    FPR = 1 - TNR\n",
    "    try:\n",
    "        F_1 = 2 * (precision * recall) / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        F_1 = 0\n",
    "    try:\n",
    "        F_2 = 5 * (precision * recall) / (4*precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        F_2 = 0\n",
    "\n",
    "    try:\n",
    "        # between -1 and 1\n",
    "        MCC = (TP*TN - FP*FN)/(np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)))\n",
    "    except ZeroDivisionError:\n",
    "        MCC = 0\n",
    "    current_metrics = {\"precision\": precision,\n",
    "                       \"accuracy\": accuracy,\n",
    "                       \"recall\": recall,\n",
    "                       \"F1_score\": F_1,\n",
    "                       \"F2_score\": F_2,\n",
    "                       \"TPR\": TPR,\n",
    "                       \"TNR\": TNR,\n",
    "                       \"FNR\": FNR,\n",
    "                       \"FPR\": FPR,\n",
    "                       \"MCC\": MCC\n",
    "                       }\n",
    "    return current_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve(metrics, file_path):\n",
    "    # TODO: fix plot saving\n",
    "    fig = plt.figure()\n",
    "    plt.plot([m[\"FPR\"] for m in metrics],\n",
    "             [m[\"TPR\"] for m in metrics])\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"FALSE POSITIVE RATE\")\n",
    "    plt.ylabel(\"TRUE POSITIVE RATE\")\n",
    "    savefig(fig, file_path)\n",
    "\n",
    "    plt.clf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_per_img(img, rec_img):\n",
    "    # mean squared error\n",
    "    return np.sum(np.power(rec_img - img, 2)) / (np.prod(img.shape) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(model, X_normal, X_anomaly, target_size):\n",
    "    samples_normal = img_generator(\n",
    "        X_normal, len(X_normal), target_size\n",
    "    )\n",
    "    decoded_samples_normal = predict_from_generator(model, samples_normal)\n",
    "    normal_losses = np.array([loss_per_img(i, ri)for i, ri in zip(\n",
    "        img_generator(X_normal, len(X_normal), target_size), decoded_samples_normal)])\n",
    "\n",
    "    decoded_samples_anomaly = predict_from_generator(model, img_generator(\n",
    "        X_anomaly, len(X_anomaly), target_size\n",
    "    ))\n",
    "    anomaly_losses = np.array([loss_per_img(i, ri)for i, ri in zip(\n",
    "        img_generator(X_anomaly, len(X_anomaly), target_size), decoded_samples_anomaly)])\n",
    "    return normal_losses, anomaly_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(model, X_normal, X_anomaly, loss_boundary, img_dir, target_size):\n",
    "    \"\"\"anomaly detection based on the loss\"\"\"\n",
    "\n",
    "    # compute losses on the test set\n",
    "    normal_losses, anomaly_losses = get_losses(model,\n",
    "                                               X_normal, X_anomaly, target_size)\n",
    "    bins = 10\n",
    "    # loss distribution over the normal dataset\n",
    "    fig = plt.figure()\n",
    "    label = \"Distribution of normal loss values\"\n",
    "    plot_hist(normal_losses, relative=True,\n",
    "              color=\"g\", bins=bins, label=label)\n",
    "\n",
    "    label = \"Distribution of normal loss values\"\n",
    "    # loss distribution over the anomaly dataset\n",
    "    plot_hist(anomaly_losses, relative=True,\n",
    "              color=\"r\", bins=bins, label=label)\n",
    "\n",
    "    savefig(fig, os.path.join(img_dir, \"loss-dist.png\"))\n",
    "    plt.clf()\n",
    "\n",
    "    # generate new samples for evaluation\n",
    "    samples_normal = np.array(\n",
    "        list(img_generator(X_normal, len(X_normal), target_size)))\n",
    "    samples_anomaly = np.array(\n",
    "        list(img_generator(X_anomaly, len(X_anomaly), target_size)))\n",
    "\n",
    "    # ground truth: positives = normality\n",
    "    TN = samples_normal[normal_losses < loss_boundary]\n",
    "    FP = samples_normal[normal_losses >= loss_boundary]\n",
    "\n",
    "    # ground truth: negatives = anomaly\n",
    "    TP = samples_anomaly[anomaly_losses >= loss_boundary]\n",
    "    FN = samples_anomaly[anomaly_losses < loss_boundary]\n",
    "\n",
    "    # ROC_curve(metrics, os.path.join(img_dir, \"loss-ROC.png\"))\n",
    "    return get_metrics(len(TP), len(TN), len(FP), len(FN))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_svm(encoder, oc_svm, score_boundary,  X_normal, X_anomaly, img_dir, target_size, batch_size):\n",
    "\n",
    "    print(\"Encoding normal test images to latent space...\")\n",
    "    encoded_normal_imgs_test = encoder.predict_generator(\n",
    "        train_img_generator(X_normal, batch_size, target_size, preproc=False), steps=len(X_normal)/batch_size)\n",
    "    print(\"Encoding anomaly test images to latent space...\")\n",
    "    encoded_anomaly_imgs_test = encoder.predict_generator(\n",
    "        train_img_generator(X_anomaly, batch_size, target_size, preproc=False), steps=len(X_anomaly)/batch_size)\n",
    "\n",
    "    # reshape to suitable shape for OCC\n",
    "    encoded_normal_imgs_test = encoded_normal_imgs_test.reshape(\n",
    "        -1, np.prod(encoded_normal_imgs_test.shape[1:]))\n",
    "    encoded_anomaly_imgs_test = encoded_anomaly_imgs_test.reshape(\n",
    "        - 1, np.prod(encoded_anomaly_imgs_test.shape[1:]))\n",
    "\n",
    "    score_normal = oc_svm.decision_function(encoded_normal_imgs_test)\n",
    "    score_anomaly = oc_svm.decision_function(encoded_anomaly_imgs_test)\n",
    "\n",
    "    bins = 10\n",
    "    # loss distribution over the normal dataset\n",
    "    fig = plt.figure()\n",
    "    label = \"Distribution of the normal svm score values\"\n",
    "    score_normal = oc_svm.decision_function(encoded_normal_imgs_test)\n",
    "    plot_hist(score_normal, relative=True,\n",
    "              color=\"g\", bins=bins, label=label)\n",
    "\n",
    "    label = \"Distribution of normal svm score values\"\n",
    "    # loss distribution over the anomaly dataset\n",
    "    plot_hist(score_anomaly, relative=True,\n",
    "              color=\"r\", bins=bins, label=label)\n",
    "\n",
    "    savefig(fig, os.path.join(img_dir, \"svm-score-dist.png\"))\n",
    "    plt.clf()\n",
    "\n",
    "    print(\"Score boundary\", score_boundary)\n",
    "    print(score_normal, \"should be greater than the score boundary\")\n",
    "    print(score_anomaly, \"should be smaller than the score boundary\")\n",
    "    # Normalities are negatives (no finding)\n",
    "    # ground truth: Normality\n",
    "    TN = len(score_normal[score_normal > score_boundary])\n",
    "    FP = len(score_normal[score_normal <= score_boundary])\n",
    "\n",
    "    # Anomalies are positives (finding)\n",
    "    # ground truth: anomaly\n",
    "    TP = len(score_anomaly[score_anomaly <= score_boundary])\n",
    "    FN = len(score_anomaly[score_anomaly > score_boundary])\n",
    "\n",
    "    # ROC_curve(metrics, os.path.join(img_dir, \"svm-ROC.png\"))\n",
    "    return get_metrics(TP, TN, FP, FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss(model, normal_paths, anomaly_paths, target_size):\n",
    "    print(\"Training the loss.\")\n",
    "    normal_losses, anomaly_losses = get_losses(\n",
    "        model, normal_paths, anomaly_paths, target_size)\n",
    "    # generate new samples of normals and anomalies to determine the best boundary on the train set\n",
    "    samples_normal = np.array(\n",
    "        list(img_generator(normal_paths, len(normal_paths), target_size)))\n",
    "    samples_anomaly = np.array(\n",
    "        list(img_generator(anomaly_paths, len(anomaly_paths), target_size)))\n",
    "\n",
    "    sorted_losses = np.sort(normal_losses)\n",
    "    best_m = -1\n",
    "    best_boundary = 0\n",
    "    step_size = 1.0 / len(normal_losses)\n",
    "    steps = np.arange(0, 1 + step_size, step_size)\n",
    "\n",
    "    for threshold in steps:\n",
    "        # loss value for detection of i*100 percent normal data points\n",
    "        loss_boundary = sorted_losses[int((len(normal_losses)-1) * threshold)]\n",
    "\n",
    "        # ground truth: positives = normality\n",
    "        TP = samples_normal[normal_losses < loss_boundary]\n",
    "        FN = samples_normal[normal_losses >= loss_boundary]\n",
    "\n",
    "        # ground truth: negatives = anomaly\n",
    "        TN = samples_anomaly[anomaly_losses >= loss_boundary]\n",
    "        FP = samples_anomaly[anomaly_losses < loss_boundary]\n",
    "\n",
    "        current_metrics = get_metrics(\n",
    "            len(TP), len(TN), len(FP), len(FN))\n",
    "\n",
    "        # less or equal since we want the biggest TP_rate (i)\n",
    "        if current_metrics[\"MCC\"] >= best_m:\n",
    "            best_m = current_metrics[\"MCC\"]\n",
    "            best_boundary = loss_boundary\n",
    "\n",
    "    return best_boundary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(model, normal_file_names, anomaly_file_names, target_size, batch_size):\n",
    "    print(\"Training the OC SVM\")\n",
    "    print(\"Encoding train images to latent space...\")\n",
    "    # encode normal instances to latent space\n",
    "    encoded_normal_imgs_train = model.predict_generator(\n",
    "        train_img_generator(normal_file_names, batch_size, target_size, preproc=False), steps=len(normal_file_names)/batch_size)  # used later for One Class Classification\n",
    "    encoded_normal_imgs_train = encoded_normal_imgs_train.reshape(\n",
    "        - 1, np.prod(encoded_normal_imgs_train.shape[1:]))\n",
    "\n",
    "    # encode anomalies to latent space\n",
    "    encoded_anomaly_imgs_train = model.predict_generator(\n",
    "        train_img_generator(anomaly_file_names, batch_size, target_size, preproc=False), steps=len(anomaly_file_names)/batch_size)  # used later for One Class Classification\n",
    "    encoded_anomaly_imgs_train = encoded_anomaly_imgs_train.reshape(\n",
    "        - 1, np.prod(encoded_anomaly_imgs_train.shape[1:]))\n",
    "\n",
    "    print(\"Fitting the OC SVM\")\n",
    "    clf = svm.OneClassSVM(gamma=\"auto\")\n",
    "    clf.fit(encoded_normal_imgs_train)\n",
    "\n",
    "    metrics = []\n",
    "    best_m = -1\n",
    "\n",
    "    # computes signed distance to the hyperplane\n",
    "\n",
    "    score_normal = clf.decision_function(encoded_normal_imgs_train)\n",
    "    score_anomaly = clf.decision_function(encoded_anomaly_imgs_train)\n",
    "\n",
    "    sorted_scores = np.sort(score_normal)\n",
    "    step_size = 1 / len(encoded_anomaly_imgs_train)\n",
    "    best_boundary = 0\n",
    "    for i in np.arange(0.1, 1 + step_size, step_size):\n",
    "\n",
    "        score_boundary = sorted_scores[int((len(score_normal)-1)*i)]\n",
    "        # ground truth: Normality\n",
    "        TP = len(score_normal[score_normal > score_boundary])\n",
    "        FN = len(score_normal[score_normal <= score_boundary])\n",
    "\n",
    "        # ground truth: anomaly\n",
    "        TN = len(score_anomaly[score_anomaly <= score_boundary])\n",
    "        FP = len(score_anomaly[score_anomaly > score_boundary])\n",
    "\n",
    "        current_metrics = get_metrics(TP, TN, FP, FN)\n",
    "        metrics.append(current_metrics)\n",
    "        if current_metrics[\"MCC\"] >= best_m:\n",
    "            best_m = current_metrics[\"MCC\"]\n",
    "            best_boundary = score_boundary\n",
    "\n",
    "    return clf, best_boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://anomaly-detection-playground-data/no_damage.txt\n"
     ]
    }
   ],
   "source": [
    "loss_boundary = train_loss(\n",
    "    ae, train_file_names, anomaly_train_file_names, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = split_ae(ae)[0]\n",
    "oc_svm, score_boundary = train_svm(encoder, train_file_names,\n",
    "                                   anomaly_train_file_names, target_size, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns metrics dictionary\n",
    "metrics = evaluate(ae, loss_boundary, oc_svm, score_boundary, normal_test_file_names,\n",
    "                   anomaly_test_file_names, args.imgdir, target_size)\n",
    "with file_io.FileIO(os.path.join(args.evaldir, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

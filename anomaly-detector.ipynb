{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    "    TensorBoard,\n",
    ")\n",
    "from keras.datasets import mnist, fashion_mnist, cifar10, cifar100\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    MaxPooling2D,\n",
    "    Reshape,\n",
    "    UpSampling2D,\n",
    ")\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, train_split=0.7, test_split=0.85):\n",
    "    \"\"\"Splits data in training and test data according to the defined boundaries.\"\"\"\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    # define boundaries for train,validation and test set at .7 and .85 % of the MNIST data set\n",
    "    x_len = len(X)\n",
    "    boundaries = [int(x_len * train_split), int(x_len * test_split)]\n",
    "\n",
    "    [X_train, X_test, X_validate] = np.split(X, boundaries)\n",
    "\n",
    "    [y_train, y_test, y_validate] = np.split(y, boundaries)\n",
    "    return (X_train, X_test, X_validate), (y_train, y_test, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(dataset, train_split=0.7, test_split=0.85):\n",
    "    \"\"\"merges training and test data set \n",
    "    :param dataset is assumed to be loaded from keras.datasets, thus the from 2-tuple(2-tuple) is assumed.\n",
    "    :return value y is in one hot encoding\n",
    "    \"\"\"\n",
    "    if not (len(dataset) == 2 and all([len(d) == 2 for d in dataset])):\n",
    "        raise ValueError(\n",
    "            \"Datset has not the correct form. Please load from keras.datasets or convert to similar form.\"\n",
    "        )\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = dataset\n",
    "\n",
    "    # divide X values bei 255.0 since MNIST data set changed such that pixel values are in [0,255]\n",
    "    X = np.concatenate((X_train, X_test)) \n",
    "    y = np.concatenate((y_train, y_test))\n",
    "    if len(X.shape) == 3: # MNIST data set\n",
    "        X = X.reshape((list(X.shape) + [1]))\n",
    "        \n",
    "    \n",
    "    # one-hot encode target columns\n",
    "    y = to_categorical(y)\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR_10 = cifar10.load_data()\n",
    "# CIFAR_100 = cifar100.load_data()\n",
    "MNIST = mnist.load_data()\n",
    "# FASHION_MNIST = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMAL = 1\n",
    "ANOMALY = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = merge_data(MNIST)\n",
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(filter):\n",
    "    return np.where(np.argmax(y,axis=1)==filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7877, 28, 28, 1), (7877, 10))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_indices = get_indices(NORMAL)\n",
    "X_normal, y_normal = X[normal_indices],y[normal_indices]\n",
    "X_normal.shape,y_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7293, 28, 28, 1), (7293, 10))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_indices = get_indices(ANOMALY)\n",
    "X_anomaly, y_anomaly = X[anomaly_indices],y[anomaly_indices]\n",
    "X_anomaly.shape,y_anomaly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_normal_train, X_normal_test, X_normal_validate), (\n",
    "    y_normal_train,\n",
    "    y_normal_test,\n",
    "    y_normal_validate,\n",
    ") = train_test_split(X_normal, y_normal)\n",
    "(X_anomaly_train, X_anomaly_test, X_anomaly_validate), (\n",
    "    y_anomaly_train,\n",
    "    y_anomaly_test,\n",
    "    y_anomaly_validate,\n",
    ") = train_test_split(X_anomaly, y_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1182, 28, 28, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normal_validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist(data):\n",
    "    plt.imshow(data.reshape((28,28)))\n",
    "    plt.gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hide_axis(subplt, x=True, y=True):\n",
    "    if x:\n",
    "        subplt.get_xaxis().set_visible(False)\n",
    "    if y:\n",
    "        subplt.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(normalities, anomalies, f_plot):\n",
    "    \"\"\"Plot 8 sample images of the normalities and anomalies\"\"\"\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    # normalities\n",
    "    n = 8\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, 8, i+1)\n",
    "        hide_axis(ax)\n",
    "        f_plot(normalities[i]) # normalities\n",
    "        ax = plt.subplot(2, 8, 8+i+1)\n",
    "        f_plot(anomalies[i]) # anomalies\n",
    "        hide_axis(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_samples(X_normal_train, X_anomaly_train, plot_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_codec_from_ae(autoencoder):\n",
    "    encoder_layer = autoencoder.get_layer(\"encoder\")\n",
    "    # this model maps an input to its encoded representation; Big image to small rep\n",
    "    encoder = Model(\n",
    "        inputs=autoencoder.input, outputs=encoder_layer.output)\n",
    "\n",
    "    # create a placeholder for an encoded (ENCODING_DIM-dimensional) input\n",
    "    encoded_input = Input(shape=encoder_layer.output_shape[1:])\n",
    "\n",
    "    # getting the middle of the autoencoder\n",
    "    start = (len(autoencoder.layers))//2\n",
    "    decoder = autoencoder.layers[-start](encoded_input)\n",
    "    # stacking the decoder layers\n",
    "    for i in range(start-1, 0, -1):\n",
    "        decoder = autoencoder.layers[-i](decoder)\n",
    "\n",
    "    # create the decoder model; \"<\": encoded(small) representation to big image\n",
    "    decoder = Model(encoded_input, decoder)\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_ae(filters, input_shape=(28,28,1), kernel=(3,3), pool_size=(2,2),color=\"gray\"):\n",
    "    \n",
    "    # this is our input placeholder\n",
    "    input_img = Input(shape=input_shape)\n",
    "\n",
    "    for f in filters:\n",
    "        encode = Conv2D(f, kernel,activation=\"relu\",padding=\"same\")(input_img if f == filters[0] else encode)\n",
    "        if f == filters[-1]:\n",
    "            decode = MaxPooling2D(pool_size, padding=\"same\",name=\"encoder\")(encode) # var is named decode to enable usage in decoder half of the auto encoder\n",
    "        else:\n",
    "            encode = MaxPooling2D(pool_size, padding=\"same\")(encode)\n",
    "    \n",
    "    filters = filters[::-1]\n",
    "    \n",
    "    for f in filters:\n",
    "        if f == filters[-1] and len(filters)%2 != 0:\n",
    "            decode = Conv2D(f, kernel,activation=\"relu\")(decode)\n",
    "        else:\n",
    "            decode = Conv2D(f, kernel,activation=\"relu\",padding=\"same\")(decode)\n",
    "        decode = UpSampling2D(pool_size)(decode)\n",
    "        \n",
    "    decode = Conv2D(1 if color == \"gray\" else 3, kernel,activation=\"softmax\",padding=\"same\")(decode)\n",
    "    \n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(inputs=input_img, outputs=decode)\n",
    "\n",
    "    encoder, decoder = get_codec_from_ae(autoencoder)\n",
    "\n",
    "    # build (aka \"compile\") the model\n",
    "    autoencoder.compile(optimizer=\"adadelta\", loss=\"binary_crossentropy\")\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder for all digits...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_37 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_120 (Conv2D)          (None, 28, 28, 18)        180       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 14, 14, 18)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_121 (Conv2D)          (None, 14, 14, 20)        3260      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 7, 7, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_122 (Conv2D)          (None, 7, 7, 16)          2896      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_123 (Conv2D)          (None, 4, 4, 2)           290       \n",
      "_________________________________________________________________\n",
      "encoder (MaxPooling2D)       (None, 2, 2, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_124 (Conv2D)          (None, 2, 2, 2)           38        \n",
      "_________________________________________________________________\n",
      "up_sampling2d_51 (UpSampling (None, 4, 4, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_125 (Conv2D)          (None, 4, 4, 16)          304       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_52 (UpSampling (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_126 (Conv2D)          (None, 8, 8, 20)          2900      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_53 (UpSampling (None, 16, 16, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_127 (Conv2D)          (None, 16, 16, 18)        3258      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_54 (UpSampling (None, 32, 32, 18)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_128 (Conv2D)          (None, 32, 32, 1)         163       \n",
      "=================================================================\n",
      "Total params: 13,289\n",
      "Trainable params: 13,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected conv2d_128 to have shape (32, 32, 1) but got array with shape (28, 28, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-3bc35d8af919>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_normal_validate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_normal_validate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearlyStopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmcp_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_lr_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     )\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected conv2d_128 to have shape (32, 32, 1) but got array with shape (28, 28, 1)"
     ]
    }
   ],
   "source": [
    "ckpt_loc = os.path.join(os.curdir, \"ckpts\", \"all-conv-ae.hdf5\")\n",
    "\n",
    "if False and os.path.isfile(ckpt_loc):\n",
    "    print(\"Loading Autoencoder for all digits from directory %s...\" % ckpt_loc)\n",
    "    all_ae = load_model(ckpt_loc)\n",
    "    all_encoder, all_decoder = get_codec_from_ae(all_ae)\n",
    "else:\n",
    "    print(\"Training Autoencoder for all digits...\")\n",
    "    all_ae, all_encoder, all_decoder = build_conv_ae(filters=[18,20,16,2])\n",
    "    earlyStopping = EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=10, verbose=1, mode=\"min\", min_delta=0.0005\n",
    "    )\n",
    "    all_ae.summary()    \n",
    "    mcp_save = ModelCheckpoint(\n",
    "        ckpt_loc, save_best_only=True, verbose=1, monitor=\"val_loss\", mode=\"min\"\n",
    "    )\n",
    "    reduce_lr_loss = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.3, patience=3, verbose=1, mode=\"min\"\n",
    "    )\n",
    "    all_ae.fit(\n",
    "        X_normal_train,\n",
    "        X_normal_train,\n",
    "        epochs=128,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        validation_data=(X_normal_validate, X_normal_validate),\n",
    "        callbacks=[earlyStopping, mcp_save, reduce_lr_loss],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fix parametrised ae building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

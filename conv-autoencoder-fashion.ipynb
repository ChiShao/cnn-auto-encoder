{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The blackcellmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext blackcellmagic\n"
     ]
    }
   ],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    "    TensorBoard,\n",
    ")\n",
    "from keras.datasets import mnist, fashion_mnist, cifar10, cifar100\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    MaxPooling2D,\n",
    "    Reshape,\n",
    "    UpSampling2D,\n",
    ")\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CIFAR_10 = cifar10.load_data()\n",
    "#CIFAR_100 = cifar100.load_data()\n",
    "#MNIST = mnist.load_data()\n",
    "FASHION_MNIST = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "ENCODING_DIM = 10\n",
    "\n",
    "# decision boundary for classifier\n",
    "THRESHOLD = 0.7\n",
    "\n",
    "# working directory\n",
    "CUR_DIR = os.path.abspath(os.path.curdir)\n",
    "\n",
    "# setting random seed for reproducable results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, train_split=0.7, test_split=0.85):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    # define boundaries for train,validation and test set at .7 and .85 % of the MNIST data set\n",
    "    x_len = len(X)\n",
    "    boundaries = [int(x_len * train_split), int(x_len * test_split)]\n",
    "\n",
    "    [X_train, X_test, X_validate] = np.split(X, boundaries)\n",
    "\n",
    "    [y_train, y_test, y_validate] = np.split(y, boundaries)\n",
    "    return (X_train, X_test, X_validate), (y_train, y_test, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, train_split=0.7, test_split=0.85):\n",
    "    \"\"\"retrieves data set and rebalances dataset, such that train=.7, test=.15 and validation=.15.\n",
    "    :param dataset is assumed to be loaded from keras.datasets, thus the from 2-tuple(2-tuple) is assumed.\"\"\"\n",
    "    if not (len(dataset) == 2 and all([len(d) == 2 for d in dataset])):\n",
    "        raise ValueError(\n",
    "            \"Datset has not the correct form. Please load from keras.datasets or convert to similar form.\"\n",
    "        )\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = dataset\n",
    "    if len(X_train.shape) == 3: # MNIST data set\n",
    "        X_train = X_train.reshape((list(X_train.shape) + [1])) /255.0\n",
    "        test_len = len(X_test)\n",
    "        X_test = X_test.reshape((list(X_test.shape) + [1])) / 255.0\n",
    "\n",
    "    # divide X values bei 255.0 since MNIST data set changed such that pixel values are in [0,255]\n",
    "    X = np.concatenate((X_train, X_test)) \n",
    "    y = np.concatenate((y_train, y_test))\n",
    "\n",
    "    (X_train, X_test, X_validate), (y_train, y_test, y_validate) = train_test_split(\n",
    "        X, y\n",
    "    )  # default: .7, .85\n",
    "    # one-hot encode target columns\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    y_validate = to_categorical(y_validate)\n",
    "\n",
    "    return (X_train, X_test, X_validate), (y_train, y_test, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, X_validate), (y_train, y_test, y_validate) = get_data(FASHION_MNIST)\n",
    "#plt.imshow(X_train[3].reshape((28,28)),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49000, 28, 28, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_data_set(X, y, fashion):\n",
    "    # parameters are training AND test data for X respectively y\n",
    "    # is assumed to be in one-hot-encoding\n",
    "    y = np.argmax(y, axis=1)\n",
    "    indices = np.where(y == fashion)\n",
    "\n",
    "    # filtering by the passed fashion. needs to be an int\n",
    "    X_fashion = X[indices]\n",
    "    y_fashion = y[indices]\n",
    "\n",
    "    y_fashion = to_categorical(y_fashion)  # array of length 2 of form [0., 1.]\n",
    "\n",
    "    # splitting into training and test set is not necessary since the data for single fashions\n",
    "    # is just used for evaluation purposes (except for \"1\")\n",
    "\n",
    "    return [X_fashion, y_fashion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 28, 28, 1), (70000, 10))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((X_train, X_test, X_validate))\n",
    "y = np.concatenate((y_train, y_test, y_validate))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fashion = 0\n",
    "# X_zero, y_zero = get_specific_data_set(X, y, fashion)\n",
    "# fashion = 8\n",
    "# X_eight, y_eight = get_specific_data_set(X, y, fashion)\n",
    "# fashion = 1\n",
    "# X_one, y_one =  get_specific_data_set(X, y, fashion)\n",
    "# fashion = 7\n",
    "# X_seven, y_seven = get_specific_data_set(X, y, fashion)\n",
    "fashion_data = [get_specific_data_set(X, y, i) for i in range(10)]\n",
    "\n",
    "all_fashions = X[np.where(np.argmax(y,axis=1)!= 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "for i in range(len(fashion_data[1][0][0]))[:8]:\n",
    "    ax = plt.subplot(2, 8, i+1)\n",
    "    plt.imshow(fashion_data[0][0][i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "for i in range(len(fashion_data[1][0][0]))[:8]:\n",
    "    ax = plt.subplot(2, 8, 8+i+1)\n",
    "    plt.imshow(fashion_data[2][0][i].reshape(28, 28))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(input_dim):\n",
    "    \"\"\"Builds classifier for classification of MNIST encoded representation.\"\"\"\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(32, activation=\"relu\", input_dim=input_dim,\n",
    "                         kernel_initializer=\"random_normal\"))\n",
    "    classifier.add(Dense(ENCODING_DIM, activation=\"softmax\",\n",
    "                         kernel_initializer=\"random_normal\"))\n",
    "\n",
    "    classifier.compile(optimizer='adam', loss='mean_squared_error',\n",
    "                       metrics=['accuracy'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_aue():\n",
    "    INPUT_SHAPE = (28, 28, 1)\n",
    "    DEFAULT_KERNEL = (3, 3)\n",
    "    DEFAULT_POOL_SIZE = (2, 2)\n",
    "    # this is our input placeholder\n",
    "    input_img = Input(shape=INPUT_SHAPE)\n",
    "    # layer between input and middle layer\n",
    "    encode = Conv2D(16, DEFAULT_KERNEL, activation=\"relu\", padding=\"same\")(input_img)\n",
    "    encode = MaxPooling2D(DEFAULT_POOL_SIZE, padding=\"same\")(encode)\n",
    "    encode = Conv2D(8, DEFAULT_KERNEL, activation=\"relu\", padding=\"same\")(encode)\n",
    "    encode = MaxPooling2D(DEFAULT_POOL_SIZE, padding=\"same\")(encode)\n",
    "    encode = Conv2D(8, DEFAULT_KERNEL, activation=\"relu\", padding=\"same\")(encode)\n",
    "    encode = MaxPooling2D(DEFAULT_POOL_SIZE, padding=\"same\")(encode)\n",
    "    encode = Conv2D(6, DEFAULT_KERNEL, activation=\"relu\", padding=\"same\")(encode)\n",
    "\n",
    "    # \"encoded\" is the encoded representation of the input, middle layer of the aue\n",
    "    encoded = MaxPooling2D(DEFAULT_POOL_SIZE, padding=\"same\", name=\"encoder\")(encode)\n",
    "\n",
    "    # layer between middle and output layer\n",
    "    decode = Conv2D(6, DEFAULT_KERNEL, activation=\"relu\", padding=\"same\")(encoded)\n",
    "    decode = UpSampling2D(DEFAULT_POOL_SIZE)(decode)\n",
    "    decode = Conv2D(8, DEFAULT_KERNEL, activation=\"relu\", padding=\"same\")(decode)\n",
    "    decode = UpSampling2D(DEFAULT_POOL_SIZE)(decode)\n",
    "    decode = Conv2D(8, DEFAULT_KERNEL, activation=\"relu\", padding=\"same\")(decode)\n",
    "    decode = UpSampling2D(DEFAULT_POOL_SIZE)(decode)\n",
    "    decode = Conv2D(16, DEFAULT_KERNEL, activation=\"relu\")(decode)\n",
    "    decode = UpSampling2D(DEFAULT_POOL_SIZE)(decode)\n",
    "    decoded = Conv2D(1, DEFAULT_KERNEL, activation=\"sigmoid\", padding=\"same\")(decode)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "\n",
    "    encoder, decoder = get_codec_from_aue(autoencoder)\n",
    "\n",
    "    # build (aka \"compile\") the model\n",
    "    autoencoder.compile(optimizer=\"adadelta\", loss=\"binary_crossentropy\")\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_codec_from_aue(autoencoder):\n",
    "    encoder_layer = autoencoder.get_layer(\"encoder\")\n",
    "    # this model maps an input to its encoded representation; Big image to small rep\n",
    "    encoder = Model(\n",
    "        inputs=autoencoder.input, outputs=encoder_layer.output)\n",
    "\n",
    "    # create a placeholder for an encoded (ENCODING_DIM-dimensional) input\n",
    "    encoded_input = Input(shape=encoder_layer.output_shape[1:])\n",
    "\n",
    "    # getting the middle of the autoencoder\n",
    "    start = (len(autoencoder.layers))//2\n",
    "    decoder = autoencoder.layers[-start](encoded_input)\n",
    "    # stacking the decoder layers\n",
    "    for i in range(start-1, 0, -1):\n",
    "        decoder = autoencoder.layers[-i](decoder)\n",
    "\n",
    "    # create the decoder model; \"<\": encoded(small) representation to big image\n",
    "    decoder = Model(encoded_input, decoder)\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array to map inidices to description of the corresponding index\n",
    "fashion_verbose = [\n",
    "    \"t-shirt\",\n",
    "    \"trousers\",\n",
    "    \"pullover\",\n",
    "    \"dress\",\n",
    "    \"coat\",\n",
    "    \"sandal\",\n",
    "    \"shirt\",\n",
    "    \"sneaker\",\n",
    "    \"bag\",\n",
    "    \"ankle boot\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**all digits auto encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder for all fashions...\n",
      "WARNING:tensorflow:From C:\\Users\\a642196\\.conda\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\a642196\\.conda\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 49000 samples, validate on 10500 samples\n",
      "Epoch 1/128\n",
      "49000/49000 [==============================] - 55s 1ms/step - loss: 0.4202 - val_loss: 0.3887\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38867, saving model to C:\\Users\\a642196\\coding\\CNN-auto-encoder\\ckpts\\fashionall-conv-ae.hdf5\n",
      "Epoch 2/128\n",
      "49000/49000 [==============================] - 54s 1ms/step - loss: 0.3555 - val_loss: 0.3421\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38867 to 0.34213, saving model to C:\\Users\\a642196\\coding\\CNN-auto-encoder\\ckpts\\fashionall-conv-ae.hdf5\n",
      "Epoch 3/128\n",
      "49000/49000 [==============================] - 54s 1ms/step - loss: 0.3416 - val_loss: 0.3350\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34213 to 0.33502, saving model to C:\\Users\\a642196\\coding\\CNN-auto-encoder\\ckpts\\fashionall-conv-ae.hdf5\n",
      "Epoch 4/128\n",
      "49000/49000 [==============================] - 58s 1ms/step - loss: 0.3345 - val_loss: 0.3338\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.33502 to 0.33383, saving model to C:\\Users\\a642196\\coding\\CNN-auto-encoder\\ckpts\\fashionall-conv-ae.hdf5\n",
      "Epoch 5/128\n",
      "49000/49000 [==============================] - 65s 1ms/step - loss: 0.3297 - val_loss: 0.3262\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.33383 to 0.32624, saving model to C:\\Users\\a642196\\coding\\CNN-auto-encoder\\ckpts\\fashionall-conv-ae.hdf5\n",
      "Epoch 6/128\n",
      "35968/49000 [=====================>........] - ETA: 19s - loss: 0.3270"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-df522eec14e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_validate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_validate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearlyStopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmcp_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_lr_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     )\n\u001b[0;32m     28\u001b[0m \u001b[0mall_ae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ckpt_loc = os.path.join(CUR_DIR, \"ckpts\", \"fashionall-conv-ae.hdf5\")\n",
    "\n",
    "if os.path.isfile(ckpt_loc):\n",
    "    print(\"Loading Autoencoder for all fashions from directory %s...\" % ckpt_loc)\n",
    "    all_ae = load_model(ckpt_loc)\n",
    "    all_encoder, all_decoder = get_codec_from_aue(all_ae)\n",
    "else:\n",
    "    print(\"Training Autoencoder for all fashions...\")\n",
    "    all_ae, all_encoder, all_decoder = build_conv_aue()\n",
    "    earlyStopping = EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=10, verbose=1, mode=\"min\", min_delta=0.0005\n",
    "    )\n",
    "    mcp_save = ModelCheckpoint(\n",
    "        ckpt_loc, save_best_only=True, verbose=1, monitor=\"val_loss\", mode=\"min\"\n",
    "    )\n",
    "    reduce_lr_loss = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.3, patience=3, verbose=1, mode=\"min\"\n",
    "    )\n",
    "    all_ae.fit(\n",
    "        X_train,\n",
    "        X_train,\n",
    "        epochs=128,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        validation_data=(X_validate, X_validate),\n",
    "        callbacks=[earlyStopping, mcp_save, reduce_lr_loss],\n",
    "    )\n",
    "all_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_train = all_ae.evaluate(X_train, X_train)\n",
    "eval_validate = all_ae.evaluate(X_validate, X_validate)\n",
    "eval_test = all_ae.evaluate(X_test, X_test)\n",
    "eval_train, eval_validate,eval_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs_train = all_encoder.predict(X_train)\n",
    "encoded_imgs_validate = all_encoder.predict(X_validate)\n",
    "encoded_imgs_test = all_encoder.predict(X_test)\n",
    "\n",
    "decoded_imgs = all_decoder.predict(encoded_imgs_test)\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.savefig(os.path.join(\"imgs\", \"all-conv-ae.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"one\" auto encoder**\n",
    "\n",
    "Learning features of the digit one. Afterwards, the distribution of the features will be computed to detect outliers which have low probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_one = digit_data[1][0]\n",
    "y_one = digit_data[1][1]\n",
    "\n",
    "(X_one_train, X_one_test, X_one_validate), (\n",
    "    y_one_train,\n",
    "    y_one_test,\n",
    "    y_one_validate,\n",
    ") = train_test_split(X_one, y_one)\n",
    "len(X_one_train), len(X_one_test), len(X_one_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_loc = os.path.join(CUR_DIR, \"ckpts\", \"%s-conv-ae.hdf5\" % digits_verbose[1])\n",
    "\n",
    "if os.path.isfile(ckpt_loc):\n",
    "    print(\"Loading Autoencoder %s from directory %s...\" % (digits_verbose[1], ckpt_loc))\n",
    "    one_ae = load_model(ckpt_loc)\n",
    "    one_encoder, one_decoder = get_codec_from_aue(one_ae)\n",
    "else:\n",
    "    print(\"Training Autoencoder for digit %s...\" % digits_verbose[1])\n",
    "    one_ae, one_encoder, one_decoder = build_conv_aue()\n",
    "    earlyStopping = EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=10, verbose=1, mode=\"min\", min_delta=0.0005\n",
    "    )\n",
    "    mcp_save = ModelCheckpoint(\n",
    "        ckpt_loc, save_best_only=True, verbose=1, monitor=\"val_loss\", mode=\"min\"\n",
    "    )\n",
    "    reduce_lr_loss = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.3, patience=3, verbose=1, mode=\"min\"\n",
    "    )\n",
    "    tb = TensorBoard(\n",
    "        log_dir=\"./ckpts\", histogram_freq=0, write_graph=True, write_images=True\n",
    "    )\n",
    "    one_ae.fit(\n",
    "        X_one_train,\n",
    "        X_one_train,\n",
    "        epochs=128,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        validation_data=(X_one_validate, X_one_validate),\n",
    "        callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tb],\n",
    "    )\n",
    "one_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_one_train = one_ae.evaluate(X_one_train, X_one_train)\n",
    "eval_one_validate = one_ae.evaluate(X_one_validate, X_one_validate)\n",
    "eval_one_test = one_ae.evaluate(X_one_test, X_one_test)\n",
    "eval_one_train, eval_one_validate, eval_one_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_one_imgs_train = one_encoder.predict(X_one_train)\n",
    "encoded_one_imgs_validate = one_encoder.predict(X_one_validate)\n",
    "encoded_one_imgs_test = one_encoder.predict(X_one_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructed \"1\" with auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_one_imgs = one_decoder.predict(encoded_one_imgs_train)\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(X_one_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_one_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.savefig(os.path.join(\"imgs\", \"one-conv-ae.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructed \"7\" (aka anomaly) with auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sevens_imgs = one_ae.predict(digit_data[7][0])\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(digit_data[7][0][i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_sevens_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.savefig(os.path.join(\"imgs\", \"seven-conv-one-ae.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection with root of squared loss per pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_per_img(img,rec_img):\n",
    "    return np.sqrt(np.sum(np.power(rec_img - img,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_losses = np.array([])\n",
    "# for img in digit_data[7][0]:\n",
    "\n",
    "\n",
    "sevens = digit_data[7][0]\n",
    "imgs = sevens.reshape(-1, 28, 28, 1)\n",
    "rec_imgs = one_ae.predict(imgs)\n",
    "seven_losses = np.array([loss_per_img(i, ri) for i, ri in zip(imgs, rec_imgs)])\n",
    "\n",
    "# imgs = all_digits#.reshape(-1,28,28,1)\n",
    "# rec_imgs = one_ae.predict(imgs)\n",
    "# all_digits_losses = np.array([loss_per_img(i,ri) for i,ri in zip(imgs,rec_imgs)])\n",
    "\n",
    "seven_losses.max(), seven_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_losses = np.array([])\n",
    "imgs = X_one_test.reshape(len(X_one_test),28,28,1)\n",
    "rec_imgs = one_ae.predict(imgs)\n",
    "normal_losses = np.array([loss_per_img(i,ri) for i,ri in zip(imgs,rec_imgs)])\n",
    "\n",
    "normal_losses.max(), normal_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss distribution over the normal dataset\n",
    "bins = 100\n",
    "t = np.linspace(0, normal_losses.max(), bins)\n",
    "hist_normal = np.histogram(normal_losses, bins=bins)\n",
    "plt.plot(t, hist_normal[0] / len(normal_losses), \"b\")\n",
    "\n",
    "# loss distribution over the anomaly dataset for digit \"seven\"\n",
    "hist_sevens = np.histogram(seven_losses, bins=bins)\n",
    "t = np.linspace(0, seven_losses.max(), bins)\n",
    "plt.plot(t, hist_sevens[0] / len(seven_losses), \"r\")\n",
    "\n",
    "# loss distribution over the anomaly dataset for all digits\n",
    "hist_all = np.histogram(all_digits_losses, bins=bins)\n",
    "t = np.linspace(0, all_digits_losses.max(), bins)\n",
    "plt.plot(t, hist_all[0] / len(all_digits_losses), \"g\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss distribution over the anomaly dataset\n",
    "bins = 100\n",
    "hist_anomaly = np.histogram(seven_losses, bins=bins)\n",
    "t = np.linspace(0, seven_losses.max(), bins)\n",
    "plt.plot(t, hist_anomaly[0], \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x:= \"false detected anomalies in %\"\n",
    "# y:= \"correct detected normal data points in %\"\n",
    "rs = []\n",
    "prec = 1000\n",
    "step_size = 1 / prec\n",
    "decs = np.arange(0.9, 1, step_size)\n",
    "for i in decs:  # i == 0.98 seems to be a valid value\n",
    "    loss_boundary = np.sort(normal_losses)[\n",
    "        int(len(normal_losses) * i)\n",
    "    ]  # loss value for detection of i% normal data points\n",
    "    x = seven_losses[\n",
    "        np.where(seven_losses < loss_boundary)\n",
    "    ]  # loss values for anomalies which are below than the boundary\n",
    "    ratio_of_undetected = len(x) / len(\n",
    "        seven_losses\n",
    "    )  # ratio between not detected loss values for anomalies\n",
    "    rs = np.append(rs, ratio_of_undetected)\n",
    "    if (int(i * prec)) % int(prec / 200) == 0:\n",
    "        print(\n",
    "            \"i:%.3f * ratio:%.3f = %.3f\"\n",
    "            % (i, ratio_of_undetected, i * ratio_of_undetected)\n",
    "        )\n",
    "plt.plot(rs, decs)  # ,zs=decs*rs)\n",
    "plt.grid()\n",
    "plt.xlabel(\"FALSE POSITIVE RATE\")\n",
    "plt.ylabel(\"TRUE POSITIVE RATE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_boundary = np.sort(normal_losses)[int(len(normal_losses) * 0.98)]\n",
    "ind_undetected_anomalies = np.where(\n",
    "    seven_losses < loss_boundary\n",
    ")  # indices for undetected anomalies in the data set\n",
    "ind_detected_normals = np.where(\n",
    "    normal_losses >= loss_boundary\n",
    ")  # indices for as anomaly detected normal data points in the data set\n",
    "\n",
    "detected_normals = X_one_test[ind_detected_normals]\n",
    "undetected_anomalies = digit_data[7][0][ind_undetected_anomalies]\n",
    "loss_boundary, len(detected_normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(undetected_anomalies)  # how many digits we will display\n",
    "plt.figure(figsize=(40, 32))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(n/5, n/4, i + 1)\n",
    "    plt.imshow(undetected_anomalies[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.savefig(os.path.join(\"imgs\", \"undetected-sevens.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display digits \"1\" which have been detectes as normaility \n",
    "n = len(detected_normals)  # how many digits we will display\n",
    "plt.figure(figsize=(40, 32))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(n/5, n/4, i + 1)\n",
    "    plt.imshow(detected_normals[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.savefig(os.path.join(\"imgs\", \"detected-ones.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_one_test[np.argmax(normal_losses)].reshape((28,28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(digit_data[7][0][np.argmax(seven_losses)].reshape((28,28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Anomaly Detector\n",
    "This anomaly detector should be able to seperate images of the handwritten digits $1$ and $7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_anomaly(ae, x):\n",
    "    x = x.reshape((1,28,28,1))\n",
    "    pred = ae.predict(x)\n",
    "    loss = np.sum(pred - x) **2 # squared loss => positive value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All digits classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = np.prod(encoded_imgs_train.shape[1:], dtype=np.int64)\n",
    "encoded_imgs_train = encoded_imgs_train.reshape(len(encoded_imgs_train), flat)\n",
    "encoded_imgs_validate = encoded_imgs_validate.reshape(\n",
    "    len(encoded_imgs_validate), flat)\n",
    "encoded_imgs_test = encoded_imgs_test.reshape(len(encoded_imgs_test), flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_loc = os.path.join(CUR_DIR, \"ckpts\", \"classifier.hdf5\")\n",
    "if os.path.isfile(ckpt_loc):\n",
    "    print(\"Loading classifier from directory %s...\" % ckpt_loc)\n",
    "    classifier = load_model(ckpt_loc)\n",
    "else:\n",
    "    print(\"Training classifier...\")\n",
    "    classifier = build_classifier(input_dim=flat)\n",
    "    earlyStopping = EarlyStopping(\n",
    "        monitor=\"val_acc\", patience=5, verbose=1, mode=\"max\", min_delta=0.0005\n",
    "    )\n",
    "    mcp_save = ModelCheckpoint(\n",
    "        ckpt_loc, save_best_only=True, verbose=1, monitor=\"val_acc\", mode=\"max\"\n",
    "    )\n",
    "    reduce_lr_loss = ReduceLROnPlateau(\n",
    "        monitor=\"val_acc\", factor=0.3, patience=3, verbose=1, mode=\"max\"\n",
    "    )\n",
    "    classifier.fit(\n",
    "        encoded_imgs_train,\n",
    "        y_train,\n",
    "        validation_data=(encoded_imgs_validate, y_validate),\n",
    "        batch_size=16,\n",
    "        epochs=32,\n",
    "        shuffle=True,\n",
    "        callbacks=[earlyStopping, mcp_save, reduce_lr_loss],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_train = classifier.evaluate(encoded_imgs_train, y_train)\n",
    "eval_validate = classifier.evaluate(encoded_imgs_validate, y_validate)\n",
    "eval_test = classifier.evaluate(encoded_imgs_test, y_test)\n",
    "eval_train,eval_validate, eval_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cm(input, y_true):\n",
    "    \"\"\"Computes confusion matrix.\"\"\"\n",
    "    y_pred = tf.argmax(classifier.predict(input), axis=1)\n",
    "    y_true = tf.argmax(y_true, axis=1)\n",
    "\n",
    "    c = tf.keras.backend.eval(y_pred)\n",
    "    d = tf.keras.backend.eval(y_true)\n",
    "\n",
    "    return confusion_matrix(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(cm):\n",
    "    results = []\n",
    "    for i in range(len(cm)):  # rows\n",
    "        TP = cm[i][i]\n",
    "        fp_tp = np.sum(cm[i])\n",
    "        results.append(TP / fp_tp)\n",
    "    return results + [np.mean(results)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(cm):\n",
    "    results = []\n",
    "    for i in range(len(cm)):  # rows\n",
    "        TP = cm[i][i]\n",
    "        tp_fn = 0\n",
    "        for j in range(len(cm[i])):\n",
    "            tp_fn += cm[j][i]\n",
    "        results.append(TP/tp_fn)\n",
    "    return results + [np.mean(results)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_train = get_cm(encoded_imgs_train, y_train)\n",
    "print(cm_train, precision(cm_train)[-1], recall(cm_train)[-1], sep=\"\\n\")\n",
    "\n",
    "cm_validate = get_cm(encoded_imgs_validate, y_validate)\n",
    "print(cm_validate, precision(cm_validate)[-1], recall(cm_validate)[-1], sep=\"\\n\")\n",
    "\n",
    "cm_test = get_cm(encoded_imgs_test, y_test)\n",
    "print(cm_test, precision(cm_test)[-1], recall(cm_test)[-1], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection with DJ CF Gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_flat = np.prod(encoded_one_imgs_train.shape[1:], dtype=np.int64)\n",
    "encoded_one_imgs_train = encoded_one_imgs_train.reshape(\n",
    "    len(encoded_one_imgs_train), one_flat\n",
    ")\n",
    "encoded_one_imgs_validate = encoded_one_imgs_validate.reshape(\n",
    "    len(encoded_one_imgs_validate), one_flat\n",
    ")\n",
    "encoded_one_imgs_test = encoded_one_imgs_test.reshape(\n",
    "    len(encoded_one_imgs_test), one_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss(x_i, my_i, sigma_i2):\n",
    "    # gaussian distribution for one feature\n",
    "    if sigma_i2 == 0:\n",
    "        return 1 if x_i == my_i else 0\n",
    "    return np.array(\n",
    "        (1 / np.sqrt(2 * np.pi * sigma_i2))\n",
    "        * np.exp(-(x_i - my_i) ** 2 / (2 * sigma_i2))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my(X):\n",
    "    return np.array([(1 / len(x)) * np.sum(x) for x in X.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma2(X, my):\n",
    "    # computes sigma squared for each feature\n",
    "    m = len(X)  # number of data points\n",
    "    return np.array([(1/m)*np.sum((X[:, i] - my[i]) ** 2) for i in range(len(my))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(X,my,sigma2):\n",
    "    return np.array([np.prod([gauss(x[i], my[i], sigma2[i])\n",
    "               for i in range(len(x))]) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = encoded_one_imgs_train\n",
    "m = len(X)\n",
    "number_of_features = len(X[0])\n",
    "my = np.array([(1 / len(x)) * np.sum(x) for x in X.T])\n",
    "sigma_2 = np.array(\n",
    "    [np.sum((X[:, i] - my[i]) ** 2) / m for i in range(number_of_features)]\n",
    ")\n",
    "\n",
    "p_all = np.sort(p(X, my, sigma_2))\n",
    "epsilon = 1e-7\n",
    "thresholded_P = p_all[np.where(p_all > epsilon)]\n",
    "len(thresholded_P) / len(p_all), my, sigma_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Encoded Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(len(my)):\n",
    "    # plots distribution of i-th feature\n",
    "    # display original\n",
    "    bins = 51\n",
    "    hist = np.histogram(X[:, i], bins=bins)\n",
    "    ax = plt.subplot(4, 6, i + 1)\n",
    "    h_min = min(hist[1])\n",
    "    h_max = max(hist[1])\n",
    "    delta = abs(h_max - h_min)\n",
    "    step = delta / (bins)\n",
    "    r = np.arange(h_min, h_max, step)\n",
    "    plt.plot(r, hist[0])\n",
    "    print(r, hist[0], step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_values_to_img(values):\n",
    "    values = np.reshape(values, (1, 2, 2, 6))\n",
    "    random_img = decoder.predict(values)\n",
    "    return np.reshape(random_img, (28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 50\n",
    "hist_all = np.histogram(p_all, bins=bins)\n",
    "h_min = min(hist_all[1])\n",
    "h_max = max(hist_all[1])\n",
    "delta = abs(h_max - h_min)\n",
    "r = np.arange(h_min, h_max, delta / bins)\n",
    "plt.plot(r, hist_all[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting fashion-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion():\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "    img = X_train[0].reshape((28, 28))\n",
    "    # reshape data to fit model\n",
    "    plt.imshow(X_train[1], cmap=\"Greys\", vmin=0, vmax=255)\n",
    "    plt.savefig(\"fashion_example_img.png\")\n",
    "\n",
    "    \"\"\"for Autoencoder\"\"\"\n",
    "    X_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))\n",
    "    X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))\n",
    "    # np.random.shuffle(X_test)\n",
    "    X_validate = X_test[len(X_test) // 2 :]\n",
    "    X_test = X_test[: len(X_test) // 2]\n",
    "\n",
    "    return X_train / 255.0, X_test / 255.0, X_validate / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fashion_train, X_fashion_test, X_fashion_validate = get_fashion()\n",
    "print(X_fashion_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colored images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dic = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(CUR_DIR, \"data\", \"cifar-10-batches-py\")\n",
    "train_batches = []\n",
    "test_batch = []\n",
    "for filename in os.listdir(data_path):\n",
    "    filename = os.path.join(data_path, filename)\n",
    "    if \"data_batch\" in filename:\n",
    "        train_batches.append(unpickle(filename))  # keys: labels, data, filenames\n",
    "    if \"test_batch\" in filename:\n",
    "        test_batch = unpickle(filename)\n",
    "len(train_batches), test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(CUR_DIR, \"data\", \"cifar-100-python\")\n",
    "train_batches = []\n",
    "test_batch = []\n",
    "for filename in os.listdir(data_path):\n",
    "    filename = os.path.join(data_path, filename)\n",
    "    if \"train\" in filename:\n",
    "        train_batches = unpickle(filename)  # keys: labels, data, filenames\n",
    "    if \"test\" in filename:\n",
    "        test_batch = unpickle(filename)\n",
    "len(train_batches), test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
